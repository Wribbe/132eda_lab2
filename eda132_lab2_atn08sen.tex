\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{enumitem}

\setlength{\parindent}{0cm}
\newcommand{\mat}[1]{$\textbf{#1}$}
\newcommand{\mmat}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{$\textbf{#1}$}
\newcommand{\mvec}[1]{\mathbf{#1}}

\begin{document}
  \begin{center}
    EDA132 -- Assignment 3 (prev. Assignment 2)\\
    \ \\
    {\Large Robot localisation with HMM based forward-filtering.} \\
    \ \\
    Stefan Eng <\texttt{atn08sen@student.lu.se}> \\
    \ \\
    \today \\
    \ \\
    ---
  \end{center}
  \vspace{-0.8cm}
  \section*{Introduction}

    The following assignment task is to locate, as precisely as possible, the
    discrete x- and y-coordinates of a hidden robot in a (X,Y)-sized
    grid-world, using the robots faulty sensor.

  \section*{Method}

    It is known that the robot orients itself according to the following
    probabilities:
    \begin{align*}
      P(\textrm{Keep orientation | Facing wall}) &= 0.0 \\
      P(\textrm{Keep orientation | $\neg$Facing wall}) &= 0.7
    \end{align*}

    There is also a known distribution of the readings coming from the faulty
    sensor:
    \begin{align*}
      P(\textrm{True position}) &= 0.1 \\
      P(\textrm{1-step-off}) &= 0.05 \cdot np1\\
      P(\textrm{2-step-off}) &= 0.025 \cdot np2\\
      P(\textrm{Nothing}) &= 1.0-\Sigma P(\textrm{\{True,1-,2-off\}})
    \end{align*}

    Where $np1$ and $np2$ are the number of possible (inside the grid)
    positions one and two steps from the robots true location respectively.
    Additionally the assumptions that the robot always successfully moves one
    tile and stays inside the grid are made. \\

    \textbf{Answer Q1:} The sensor model could be used in order to stay away
    from the walls by assuming that increased nothing-readings indicate that
    the robot is approaching the walls and corners. \\

    \textbf{Encoding:} Since there are four possible directions; N, E, S, W
    and X$\cdot$Y number of tiles there are X${\cdot}$Y${\cdot}4$ possible
    states on the grid. These states will be represented as a list of numbers
    in Python according to the following scheme.

    \begin{center}
      \texttt{grid = [0 1 2 3 4 5 6 7 ...\ X$\cdot$Y$\cdot$4-1] }
    \end{center}

    Where the digits at position 0-3 represent the values of
    \texttt{grid[0][0][N,E,S,W]}, and 4-7 the values of
    \texttt{grid[0][1][N,E,S,W]}, etc. for \texttt{grid[x][y][direction]}. \\

    \textbf{Transitions:} In order to track all possible transition-states for
    all tiles on the board, a matrix of size (X${\cdot}$Y${\cdot}4$)$^2$ is
    needed. This is represented as a list in the same manner as above, but each
    positions contains another list of the same size rather than a value. \\

    \textbf{Observations:} If the strict mathematical definition of the
    matrix-based forward filtering from the book should be followed, the
    observations need to be represented as a diagonal matrix with a diagonal
    length of (X${\cdot}$Y${\cdot}4$). There is however some wiggle-room in
    regards to the storing of the observation-probability values as long as the
    calculations are done as if they were diagonal matrices. This means that the
    diagonal values can be stored without the surrounding zeroes, which results
    in a structure equal to the transitions-representation.
    The only difference between the Observation and Transition structures will
    be an added extra matrix at the end of the Observation matrix, representing
    the probability of the sensor returning nothing. \\

    \textbf{Forward-message and HMM:} Using a Hidden-Markov-Model, there
    needs to be a single object representing the whole state of the system (the
    probability of the robot being at a tile with a given orientation);
    \vec{f}. The states are represented as a list of values in the same way as
    \texttt{grid}, shown previously. This list will also serves as the forward
    message in the forward-filtering procedure used to track the robot.

  \section*{Results}

    \textbf{Forward-filtering:} Given the transition model; \mat{T}, sensor
    model \mat{O}; and current system-state \vec{f} (all values
    $= 1.0/(X{\cdot}Y{\cdot}4)$ initially), the forward filtering for the next
    state (t+1) is done as:
    \begin{align*}
      \mvec{f}_{1:t+1} &= {\alpha}\mmat{O}_{t+1}\mmat{T}^\top\mvec{f}_{1:t}
    \end{align*}

    Where \mat{O}$_{t+1}$ is the probability diagonal representing the current
    sensor reading, $\alpha$ is a normalization constant, and $\mvec{f}_{1:t}$
    is the accumulation of all the state-representations from the initial start
    (1) to the previous iteration (t).

  \section*{Discussion}

    \textbf{Answer Q2:} By counting the number of robot movements and tracking
    attempts together with a sum of the Manhattan-distance between the
    best-guess and the true location at each attempt, the average detection
    offset is:

    \begin{center}
      $\textrm{avgManhattan}_{t+1} = \frac{\sum_{1}^{t+1} guess_t}{\#guesses}$
    \end{center}

    for the current implementation, this values hovers around 1.6 after
    $\sim$100 guesses with the lowest observed value being 1.46. In other
    words, in order to get to the correct position of the robot from a guess,
    an axis-bound movement of less than two steps would be needed, on average.

  \section*{Implementation}

    Files for the assignment are located the following network path:
    \begin{center}
      \texttt{/h/d6/tn08se5/eda132}
    \end{center}
    which contain two files written in Python(3.x); \texttt{hmm.py} and
    \texttt{viewer.py}. The command \texttt{chmod -R 777 eda132} has been run in
    order to grant all possible permissions for the directory and its
    substructures. \\

    Run the application by opening a terminal, cd'ing into the eda132 directory
    running the follwing command:
    \begin{center}
      \texttt{./hmm.py}
    \end{center}
    which will start the visualization of the tracking. It should be possible
    to run it with \texttt{python3 hmm.py} for the same effect, but take care
    to use the correct version of the Python interpreter. \\

    \textbf{viewer.py:} The text-based viewer is written using the built-in
    \texttt{print()} method, and should work in any environment that can produce
    Python textual output. Quick rundown of interface commands;
    \begin{description}[align=right,labelwidth=3cm]
      \item[q] quit viewer
      \item[t] next mode (wraps around)
      \item[<Enter>] finalize input or if no input, advance state of view
    \end{description}

\end{document}
